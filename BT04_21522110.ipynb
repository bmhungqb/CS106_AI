{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "KwpU8p2Go2mH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhSyhfEy4XSD"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython import display\n",
        "from prettytable import PrettyTable"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function"
      ],
      "metadata": {
        "id": "ZhdiispCpA53"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWLnvY7VBvIZ"
      },
      "source": [
        "def play(env, policy, render=False):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        if render:\n",
        "            env.render()\n",
        "            time.sleep(0.2)\n",
        "            if not done:\n",
        "                display.clear_output(wait=True)\n",
        "        state = next_state\n",
        "\n",
        "    return (total_reward, steps)"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU8Q1qMxD6Po"
      },
      "source": [
        "def play_multiple_times(env, policy, max_episodes):\n",
        "    success = 0\n",
        "    list_of_steps = []\n",
        "    for i in range(max_episodes):\n",
        "        total_reward, steps = play(env, policy)\n",
        "\n",
        "        if total_reward > 0:\n",
        "            success += 1\n",
        "            list_of_steps.append(steps)\n",
        "\n",
        "    print(f'Number of successes: {success}/{max_episodes}')\n",
        "    print(f'Average number of steps: {np.mean(list_of_steps)}')\n",
        "    return success/max_episodes"
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_extraction(env, v_values, gamma=0.9):\n",
        "    # initialize\n",
        "    policy = np.zeros(env.observation_space.n)\n",
        "\n",
        "    # loop through each state in the environment\n",
        "    for state in range(env.observation_space.n):\n",
        "        q_values = []\n",
        "        # loop through each action\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            # loop each possible outcome\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * v_values[next_state])\n",
        "            \n",
        "            q_values.append(q_value)\n",
        "        \n",
        "        # select the best action\n",
        "        best_action = np.argmax(q_values)\n",
        "        policy[state] = best_action\n",
        "    return policy"
      ],
      "metadata": {
        "id": "xpkdtI-heRw8"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(env, policy, max_iters=500, gamma=0.9):\n",
        "    # Initialize the values of all states to be 0\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # Update the value of each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            action = policy[state]\n",
        "\n",
        "            # Compute the q-value of the action\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "\n",
        "            v_values[state] = q_value # update v-value\n",
        "        \n",
        "        # Check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            print(f'Converged at {i}-th iteration.')\n",
        "            break\n",
        "    \n",
        "    return v_values"
      ],
      "metadata": {
        "id": "7CM1fuKKeS1F"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Value iteration"
      ],
      "metadata": {
        "id": "pqzoTvQ2pI_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "runtimes_ValueIter = []\n",
        "winrates_ValueIter = []"
      ],
      "metadata": {
        "id": "yorUOkMYqzCn"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(env, max_iters=500, gamma=0.9):\n",
        "  # initialize\n",
        "  v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "  for i in range(max_iters):\n",
        "    prev_v_values = np.copy(v_values)\n",
        "\n",
        "      # update the v-value for each state\n",
        "    for state in range(env.observation_space.n):\n",
        "      q_values = []\n",
        "      \n",
        "      # compute the q-value for each action that we can perform at the state\n",
        "      for action in range(env.action_space.n):\n",
        "        q_value = 0\n",
        "        # loop through each possible outcome\n",
        "        for prob, next_state, reward, done in env.P[state][action]:\n",
        "            q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "        \n",
        "        q_values.append(q_value)\n",
        "      \n",
        "        # select the max q-values\n",
        "        best_action = np.argmax(q_values)\n",
        "        v_values[state] = q_values[best_action]\n",
        "\n",
        "    # check convergence\n",
        "    if np.all(np.isclose(v_values, prev_v_values)):\n",
        "      print(f'Converged at {i}-th iteration.')\n",
        "      break\n",
        "    \n",
        "  return v_values"
      ],
      "metadata": {
        "id": "cvTnh1h4pQ5y"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'FrozenLake-v1'"
      ],
      "metadata": {
        "id": "i1lXNK6ppqCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v1')\n",
        "print(\"observation_space: \",env.observation_space.n)\n",
        "print(\"action_space: \",env.action_space.n)\n",
        "print('----------------------------')\n",
        "start = time.time()\n",
        "optimal_v_values = value_iteration(env, max_iters=500, gamma=0.9)\n",
        "end = time.time()\n",
        "\n",
        "optimal_policy = policy_extraction(env, optimal_v_values, gamma=0.9)\n",
        "winrate = play_multiple_times(env, optimal_policy, 1000)\n",
        "\n",
        "runtimes_ValueIter.append(end-start)\n",
        "winrates_ValueIter.append(winrate)\n",
        "print(\"Runtime: \", end-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Os3nphEap_K5",
        "outputId": "a113c5ea-43bd-4249-f5d1-146af3d0b667"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation_space:  16\n",
            "action_space:  4\n",
            "----------------------------\n",
            "Converged at 79-th iteration.\n",
            "Number of successes: 720/1000\n",
            "Average number of steps: 36.423611111111114\n",
            "Runtime:  0.11992216110229492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'FrozenLake8x8-v0'"
      ],
      "metadata": {
        "id": "AiM1RlEEsek2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake8x8-v1')\n",
        "print(\"observation_space: \",env.observation_space.n)\n",
        "print(\"action_space: \",env.action_space.n)\n",
        "print('----------------------------')\n",
        "start = time.time()\n",
        "optimal_v_values = value_iteration(env, max_iters=500, gamma=0.9)\n",
        "end = time.time()\n",
        "\n",
        "optimal_policy = policy_extraction(env, optimal_v_values, gamma=0.9)\n",
        "winrate = play_multiple_times(env, optimal_policy, 1000)\n",
        "\n",
        "runtimes_ValueIter.append(end-start)\n",
        "winrates_ValueIter.append(winrate)\n",
        "print(\"Runtime: \", end-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-PKGNOTsLAc",
        "outputId": "4c471bdc-1523-4c82-8a4a-142a34ceeaea"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation_space:  64\n",
            "action_space:  4\n",
            "----------------------------\n",
            "Converged at 117-th iteration.\n",
            "Number of successes: 732/1000\n",
            "Average number of steps: 73.73224043715847\n",
            "Runtime:  0.7215578556060791\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'Taxi-v3'"
      ],
      "metadata": {
        "id": "ygtcixtKs18j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('Taxi-v3')\n",
        "print(\"observation_space: \",env.observation_space.n)\n",
        "print(\"action_space: \",env.action_space.n)\n",
        "print('----------------------------')\n",
        "start = time.time()\n",
        "optimal_v_values = value_iteration(env, max_iters=500, gamma=0.9)\n",
        "end = time.time()\n",
        "\n",
        "optimalPolicy = policy_extraction(env, optimal_v_values, gamma=0.9)\n",
        "winrate = play_multiple_times(env, optimalPolicy, 1000)\n",
        "\n",
        "runtimes_ValueIter.append(end-start)\n",
        "winrates_ValueIter.append(winrate)\n",
        "print(\"Runtime: \", end-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2VyzbHEswZp",
        "outputId": "4debf240-f80b-4d85-8f05-8c8cb8408b54"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation_space:  500\n",
            "action_space:  6\n",
            "----------------------------\n",
            "Converged at 116-th iteration.\n",
            "Number of successes: 1000/1000\n",
            "Average number of steps: 12.989\n",
            "Runtime:  5.267210006713867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy Iteration"
      ],
      "metadata": {
        "id": "_vD_MwFYtTBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(env, max_iters=500, gamma=0.9):\n",
        "    policy = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_policy = np.copy(policy)\n",
        "\n",
        "        # Policy Evaluation\n",
        "        v_values = policy_evaluation(env, policy, max_iters=max_iters, gamma=gamma)\n",
        "\n",
        "        # Policy Improvement\n",
        "        policy = policy_extraction(env, v_values, gamma=gamma)\n",
        "\n",
        "        # Check convergence\n",
        "        is_converged = True\n",
        "        for state in range(env.observation_space.n):\n",
        "            if policy[state] != prev_policy[state]:\n",
        "                is_converged = False\n",
        "                break\n",
        "\n",
        "        if is_converged:\n",
        "            print(f'Converged at {i}-th iteration.')\n",
        "            break\n",
        "    return policy"
      ],
      "metadata": {
        "id": "8iniLAsht3lq"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runtimes_PolicyIter = []\n",
        "winrates_PolicyIter = []"
      ],
      "metadata": {
        "id": "uA7D4FzEs9Gg"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'FrozenLake-v1'"
      ],
      "metadata": {
        "id": "lAYnD54IxMgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the environment\n",
        "env = gym.make('FrozenLake-v1')\n",
        "print(\"observation_space:\", env.observation_space.n)\n",
        "print(\"action_space:\", env.action_space.n)\n",
        "print('----------------------------')\n",
        "\n",
        "# Perform policy iteration\n",
        "start = time.time()\n",
        "optimalPolicy = policy_iteration(env, max_iters=500, gamma=0.9)\n",
        "end = time.time()\n",
        "print(\"Runtime:\", end - start)\n",
        "\n",
        "# Play multiple times and calculate win rate\n",
        "winrate = play_multiple_times(env, optimalPolicy, 1000)\n",
        "\n",
        "runtimes_PolicyIter.append(end - start)\n",
        "winrates_PolicyIter.append(winrate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMc38XGCt-V6",
        "outputId": "1e54ce24-26bb-4dd7-c559-58f76c37a9cd"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation_space: 16\n",
            "action_space: 4\n",
            "----------------------------\n",
            "Converged at 0-th iteration.\n",
            "Converged at 23-th iteration.\n",
            "Converged at 59-th iteration.\n",
            "Converged at 62-th iteration.\n",
            "Converged at 79-th iteration.\n",
            "Converged at 80-th iteration.\n",
            "Converged at 5-th iteration.\n",
            "Runtime: 0.07497310638427734\n",
            "Number of successes: 730/1000\n",
            "Average number of steps: 37.487671232876714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'FrozenLake8x8-v1'"
      ],
      "metadata": {
        "id": "RFDKeuyexStu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the environment\n",
        "env = gym.make('FrozenLake8x8-v1')\n",
        "print(\"observation_space:\", env.observation_space.n)\n",
        "print(\"action_space:\", env.action_space.n)\n",
        "print('----------------------------')\n",
        "\n",
        "# Perform policy iteration\n",
        "start = time.time()\n",
        "optimalPolicy = policy_iteration(env, max_iters=500, gamma=0.9)\n",
        "end = time.time()\n",
        "print(\"Runtime:\", end - start)\n",
        "\n",
        "# Play multiple times and calculate win rate\n",
        "winrate = play_multiple_times(env, optimalPolicy, 1000)\n",
        "runtimes_PolicyIter.append(end - start)\n",
        "winrates_PolicyIter.append(winrate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvptfqLKuOLq",
        "outputId": "780beac8-31b2-4c4d-ef66-9a278ae69ad0"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation_space: 64\n",
            "action_space: 4\n",
            "----------------------------\n",
            "Converged at 27-th iteration.\n",
            "Converged at 91-th iteration.\n",
            "Converged at 92-th iteration.\n",
            "Converged at 86-th iteration.\n",
            "Converged at 90-th iteration.\n",
            "Converged at 92-th iteration.\n",
            "Converged at 95-th iteration.\n",
            "Converged at 100-th iteration.\n",
            "Converged at 112-th iteration.\n",
            "Converged at 117-th iteration.\n",
            "Converged at 9-th iteration.\n",
            "Runtime: 0.4588966369628906\n",
            "Number of successes: 741/1000\n",
            "Average number of steps: 70.26585695006747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'Taxi-v3'"
      ],
      "metadata": {
        "id": "m2wLFXtIxc2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the environment\n",
        "env = gym.make('Taxi-v3')\n",
        "print(\"observation_space:\", env.observation_space.n)\n",
        "print(\"action_space:\", env.action_space.n)\n",
        "print('----------------------------')\n",
        "\n",
        "# Perform policy iteration\n",
        "start = time.time()\n",
        "optimalPolicy = policy_iteration(env, max_iters=500, gamma=0.9)\n",
        "end = time.time()\n",
        "print(\"Runtime:\", end - start)\n",
        "\n",
        "# Play multiple times and calculate win rate\n",
        "winrate = play_multiple_times(env, optimalPolicy, 1000)\n",
        "\n",
        "runtimes_PolicyIter.append(end - start)\n",
        "winrates_PolicyIter.append(winrate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohhfdgtZxVUg",
        "outputId": "af206b4a-62ca-44ed-bf81-c1fdc7264c88"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation_space: 500\n",
            "action_space: 6\n",
            "----------------------------\n",
            "Converged at 88-th iteration.\n",
            "Converged at 97-th iteration.\n",
            "Converged at 100-th iteration.\n",
            "Converged at 101-th iteration.\n",
            "Converged at 102-th iteration.\n",
            "Converged at 103-th iteration.\n",
            "Converged at 106-th iteration.\n",
            "Converged at 109-th iteration.\n",
            "Converged at 110-th iteration.\n",
            "Converged at 111-th iteration.\n",
            "Converged at 112-th iteration.\n",
            "Converged at 115-th iteration.\n",
            "Converged at 116-th iteration.\n",
            "Converged at 116-th iteration.\n",
            "Converged at 116-th iteration.\n",
            "Converged at 116-th iteration.\n",
            "Converged at 116-th iteration.\n",
            "Converged at 16-th iteration.\n",
            "Runtime: 6.607874393463135\n",
            "Number of successes: 1000/1000\n",
            "Average number of steps: 12.971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclution"
      ],
      "metadata": {
        "id": "9d8JkaQPySA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#runtime\n",
        "table = PrettyTable()\n",
        "table.field_names = [\"Algorithms\", \"FrozenLake-v1\", \"'FrozenLake8x8-v1\", \"Taxi-v3\"]\n",
        "table.add_row([\"Value Iteration\", runtimes_ValueIter[0], runtimes_ValueIter[1], runtimes_ValueIter[2]])\n",
        "table.add_row([\"Policy Iteration\", runtimes_PolicyIter[0], runtimes_PolicyIter[1], runtimes_PolicyIter[2]])\n",
        "\n",
        "title = \"RUNTIME\"\n",
        "title_width = len(table.get_string(fields=[\"Algorithms\", \"FrozenLake-v1\", \"'FrozenLake8x8-v1\", \"Taxi-v3\"]).splitlines()[0])\n",
        "print(f\"{title:^{title_width}}\")\n",
        "print(table)\n",
        "# winrate\n",
        "table = PrettyTable()\n",
        "table.field_names = [\"Algorithms\", \"FrozenLake-v1\", \"'FrozenLake8x8-v1\", \"Taxi-v3\"]\n",
        "table.add_row([\"Value Iteration\", winrates_ValueIter[0], winrates_ValueIter[1], winrates_ValueIter[2]])\n",
        "table.add_row([\"Policy Iteration\", winrates_PolicyIter[0], winrates_PolicyIter[1], winrates_PolicyIter[2]])\n",
        "\n",
        "title = \"WINRATE\"\n",
        "title_width = len(table.get_string(fields=[\"Algorithms\", \"FrozenLake-v1\", \"'FrozenLake8x8-v1\", \"Taxi-v3\"]).splitlines()[0])\n",
        "print(f\"{title:^{title_width}}\")\n",
        "print(table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPsFOH6b1pYg",
        "outputId": "b8e93c11-eb03-42bb-9b21-e00cf62a8826"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      RUNTIME                                      \n",
            "+------------------+---------------------+--------------------+-------------------+\n",
            "|    Algorithms    |    FrozenLake-v1    | 'FrozenLake8x8-v1  |      Taxi-v3      |\n",
            "+------------------+---------------------+--------------------+-------------------+\n",
            "| Value Iteration  | 0.11992216110229492 | 0.7215578556060791 | 5.267210006713867 |\n",
            "| Policy Iteration | 0.07497310638427734 | 0.4588966369628906 | 6.607874393463135 |\n",
            "+------------------+---------------------+--------------------+-------------------+\n",
            "                             WINRATE                              \n",
            "+------------------+---------------+-------------------+---------+\n",
            "|    Algorithms    | FrozenLake-v1 | 'FrozenLake8x8-v1 | Taxi-v3 |\n",
            "+------------------+---------------+-------------------+---------+\n",
            "| Value Iteration  |      0.72     |       0.732       |   1.0   |\n",
            "| Policy Iteration |      0.73     |       0.741       |   1.0   |\n",
            "+------------------+---------------+-------------------+---------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comment**\n",
        " \n",
        "*   Winrate của 2 algorithms trong 3 toy games đều gần giống nhau\n",
        "*   Runtime của 2 algorithms trong 3 toy games cũng gần tương đương nhau(có sự chênh lệnh tùy vào mỗi toy game)"
      ],
      "metadata": {
        "id": "F809Ot2k24AM"
      }
    }
  ]
}